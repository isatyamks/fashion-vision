{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf69572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cede1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"../data/videos/2025-06-02_11-31-19_UTC.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e75bd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pth = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074e29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1887be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\isatyamks\\Repositories\\Stealth\\flickd\\notebooks\\..\\data\\videos\\2025-06-02_11-31-19_UTC.jpg: 640x384 1 Corporate_Top, 70.9ms\n",
      "Speed: 2.9ms preprocess, 70.9ms inference, 197.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "results = model(data)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529b35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    r.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673da45",
   "metadata": {},
   "source": [
    "# working on Data 20 \n",
    "\n",
    "\n",
    "Why YOLOv8 on COCO Is Not Enough\n",
    "The default YOLOv8 models trained on COCO dataset detect only coarse categories like:\n",
    "\n",
    "person\n",
    "\n",
    "handbag\n",
    "\n",
    "tie\n",
    "\n",
    "backpack\n",
    "\n",
    "‚ùå It does not detect fine-grained fashion items like:\n",
    "\n",
    "dress, shirt, jacket, pants, hat, heels, etc.\n",
    "\n",
    "‚úÖ What You Need Instead: Fashion-Specific Models\n",
    "To detect fashion items on people, use one of these:\n",
    "\n",
    "ü•á Option 1: Use a Pretrained Model Trained on Fashion Dataset\n",
    "üîó 1. DeepFashion2 + YOLOv8\n",
    "DeepFashion2 is a large dataset of people wearing labeled fashion items.\n",
    "\n",
    "You can train or fine-tune YOLOv8 on this dataset to detect:\n",
    "\n",
    "13 fashion categories (tops, skirts, pants, jackets, sunglasses, etc.)\n",
    "\n",
    "Keypoints & segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0245b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = \"kaggle_data/low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17fafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"../../data_20/runs/detect/fashion-v12/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb833ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c447060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  36.5724,  960.0439,  703.6796, 1279.6333]], device='cuda:0') tensor([0.3665], device='cuda:0') tensor([6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for box in results[0].boxes:\n",
    "    print(box.xyxy, box.conf, box.cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50a42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(img_pth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63a7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9e6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\isatyamks\\Repositories\\Stealth\\flickd\\notebooks\\..\\data\\videos\\2025-06-02_11-31-19_UTC.jpg: 640x384 1 Corporate_Top, 206.9ms\n",
      "Speed: 3.7ms preprocess, 206.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "results = model(img_pth)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06bd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1492d0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Casual_Jeans',\n",
       " 1: 'Casual_Sneakers',\n",
       " 2: 'Casual_Top',\n",
       " 3: 'Corporate_Gown',\n",
       " 4: 'Corporate_Shoe',\n",
       " 5: 'Corporate_Skirt',\n",
       " 6: 'Corporate_Top',\n",
       " 7: 'Corporate_Trouser',\n",
       " 8: 'Native',\n",
       " 9: 'Shorts',\n",
       " 10: 'Suits',\n",
       " 11: 'Tie'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d20a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "436c8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"crops\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "964116b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (box, conf, cls) in enumerate(zip(results.boxes.xyxy, results.boxes.conf, results.boxes.cls)):\n",
    "    if conf < conf_threshold:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e763b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = map(int, box)\n",
    "cropped = image.crop((x1, y1, x2, y2))\n",
    "class_id = int(cls.item())\n",
    "class_name = names[class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88c4283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_path = f\"crops/{class_name}_{i}_conf{conf:.2f}.jpg\"\n",
    "cropped.save(crop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cce1e8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: crops/Corporate_Top_0_conf0.37.jpg\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved: {crop_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
